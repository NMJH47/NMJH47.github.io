<!DOCTYPE html> 
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project 5b - Guanyou Li</title>
    <!-- Include MathJax for rendering mathematical formulas -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f0f0f0;
        }
        h1, h2, h3 {
            text-align: center;
        }
        section {
            margin: 20px auto;
            padding: 20px;
            max-width: 1200px;
            background-color: #ffffff;
            border-radius: 10px;
        }
        p {
            text-align: justify;
            line-height: 1.6;
        }
        .image-container {
            display: flex;
            justify-content: center;
            margin-top: 20px;
            flex-wrap: wrap;
        }
        .image-wrapper {
            width: 30%;
            margin: 10px;
        }
        .image-wrapper img {
            max-width: 100%;
            height: auto;
        }
        .caption {
            text-align: center;
            font-style: italic;
            color: #555;
        }
        footer {
            text-align: center;
            margin: 40px 0;
            font-size: 14px;
            color: #aaa;
        }
    </style>
</head>
<body>

    <h1>Project 5b - Guanyou Li</h1>

    <section>
        <h2>Part 1</h2>
        <p>
            I aimed to implement and train a single-step denoising U-Net to remove Gaussian noise from images. The U-Net architecture is well-suited for this task due to its ability to capture both local and global features through its encoder-decoder structure with skip connections.

            The implementation starts by defining the fundamental components of the U-Net model. The encoder part of the network is responsible for downsampling the input image to capture high-level features. This is achieved using convolutional layers with stride and padding to reduce the spatial dimensions while increasing the depth of the feature maps. Mathematically, a convolution operation applies a kernel over the input feature map to produce an output feature map, effectively extracting features such as edges and textures.

            In the encoder, each convolutional layer is followed by a non-linear activation function, specifically the Gaussian Error Linear Unit (GELU). The GELU activation function is defined as:

            \[
            \text{GELU}(x) = x \cdot \Phi(x)
            \]

            where \( \Phi(x) \) is the cumulative distribution function of the standard normal distribution. This activation function allows the network to weight inputs by their significance, improving the model's ability to learn complex patterns.

            The decoder part of the U-Net performs upsampling to restore the original spatial dimensions of the image. Transposed convolutional layers, also known as deconvolutions, are used for upsampling. These layers mathematically reverse the downsampling process by learning how to map low-resolution feature maps back to higher resolutions. The decoder mirrors the encoder's structure, and at each level, it incorporates skip connections from the corresponding encoder layer. These skip connections concatenate the feature maps from the encoder to the decoder, preserving spatial information that might have been lost during downsampling.

            To train the denoising model, I generated noisy images by adding Gaussian noise to clean images. This process is mathematically represented as:

            \[
            \tilde{x} = x + \sigma \cdot \epsilon
            \]

            where \( x \) is the original clean image, \( \sigma \) is the standard deviation of the noise, and \( \epsilon \) is a random tensor sampled from a standard normal distribution \( \mathcal{N}(0, 1) \). The noisy image \( \tilde{x} \) serves as the input to the model, while the original image \( x \) is the target output.

            The loss function used for training is the Mean Squared Error (MSE), which measures the average squared difference between the estimated values and the actual value:

            \[
            \mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} \left\| f(\tilde{x}_i) - x_i \right\|^2
            \]

            where \( N \) is the number of samples, \( f(\tilde{x}_i) \) is the model's output for the \( i \)-th noisy input, and \( x_i \) is the corresponding clean image. The MSE loss encourages the model to produce outputs that are as close as possible to the original images.

            For optimization, I used the Adam optimizer, which is an adaptive learning rate optimization algorithm that computes individual learning rates for different parameters. It combines the advantages of two other extensions of stochastic gradient descent: adaptive gradient algorithm (AdaGrad) and root mean square propagation (RMSProp).

            Throughout the training process, I monitored the model's performance by visualizing the denoised images alongside the original and noisy images. This qualitative assessment helped in understanding how well the model was learning to remove noise. Additionally, testing the model with different levels of noise provided insights into its robustness and generalization capabilities.

            Implementing this denoising U-Net involved a deep understanding of convolutional neural networks and their mathematical foundations. By leveraging convolutional operations, activation functions like GELU, and optimization algorithms, the model effectively learned to map noisy images back to their clean counterparts. This project not only demonstrated the practical applications of deep learning in image restoration but also provided a solid foundation for more advanced tasks such as image segmentation and generative modeling.
        </p>
        <!-- Images for Part 1 -->
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part1/1.1.png" alt="Part 1 Image 1">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part1/1.2.png" alt="Part 1 Image 2">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part1/1.3.png" alt="Part 1 Image 3">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part1/1.4.png" alt="Part 1 Image 4">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part1/1.5.png" alt="Part 1 Image 5">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part1/1.6.png" alt="Part 1 Image 6">
            </div>
        </div>
    </section>

    <section>
        <h2>Part 2</h2>
        <p>
            <strong>Part 2: Training a DDPM Denoising U-Net</strong>

            As a developer exploring generative modeling techniques, I focused on implementing and training a Denoising Diffusion Probabilistic Model (DDPM) using a U-Net architecture. The aim was to create a model capable of generating high-quality images by learning to reverse a diffusion process that progressively adds noise to data. This involved refactoring the standard unconditional U-Net to incorporate time and class conditioning mechanisms suitable for the DDPM framework, as well as implementing the mathematical formulations of both the forward and reverse diffusion processes.

            To adapt the U-Net for the DDPM, I integrated conditioning on the diffusion time steps and, optionally, on class labels. This modification allows the network to model the data distribution over the diffusion process effectively. Time conditioning was achieved by embedding each diffusion time step into a higher-dimensional representation using fully connected layers. This enables the network to adjust its computations based on the progression of the diffusion process. Similarly, class labels were one-hot encoded and passed through fully connected layers to produce class embeddings, allowing for class-conditional generation.

            These embeddings were used to modulate the feature maps within the U-Net at various stages. Specifically, the time and class embeddings were employed to scale and shift the feature maps, effectively guiding the network's output based on the current time step and desired class. Mathematically, if \( h \) represents the feature map, and \( e_t \) and \( e_c \) are the time and class embeddings, the modulated feature map \( h' \) can be expressed as:

            \[
            h' = h \odot e_c + e_t
            \]

            where \( \odot \) denotes element-wise multiplication. This modulation allows the network to incorporate temporal and class information directly into its computations, which is crucial for accurately modeling the reverse diffusion process.

            Implementing the DDPM required defining both the forward diffusion process and the reverse denoising process. In the forward process, noise is gradually added to the data over a sequence of time steps according to a predefined schedule of noise levels \( \beta_t \). The forward process can be expressed recursively using Gaussian transitions:

            \[
            q(x_t | x_{t-1}) = \mathcal{N}\left( x_t; \sqrt{1 - \beta_t} \, x_{t-1}, \beta_t \, \mathbf{I} \right)
            \]

            This allows sampling of a noisy version of the original data \( x_0 \) at any time step \( t \) directly:

            \[
            x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon
            \]

            where:

            - \( \alpha_t = 1 - \beta_t \)
            - \( \bar{\alpha}_t = \prod_{s=1}^t \alpha_s \)
            - \( \epsilon \sim \mathcal{N}(0, \mathbf{I}) \)

            The reverse diffusion process aims to recover \( x_0 \) from \( x_T \) by learning the conditional probabilities \( p_\theta(x_{t-1} | x_t) \). The U-Net model is trained to predict the noise \( \epsilon \) added at each step, rather than directly predicting \( x_{t-1} \). The training objective minimizes the expected mean squared error between the predicted noise and the true noise:

            \[
            L(\theta) = \mathbb{E}_{x_0, t, \epsilon} \left[ \left\| \epsilon - \epsilon_\theta(x_t, t, c) \right\|^2 \right]
            \]

            where \( \epsilon_\theta(x_t, t, c) \) is the model's prediction of the noise given the noisy input \( x_t \), time step \( t \), and optional class label \( c \).

            During sampling, the reverse process is performed iteratively, starting from pure noise \( x_T \) and progressively denoising it to obtain \( x_0 \). At each step \( t \), the model predicts the noise component \( \epsilon_\theta(x_t, t, c) \), which is then used to compute the mean \( \mu_t \) of the posterior distribution:

            \[
            \mu_t = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \, \epsilon_\theta(x_t, t, c) \right)
            \]

            A new sample \( x_{t-1} \) is then drawn from a normal distribution centered at \( \mu_t \) with variance \( \sigma_t^2 \), effectively reversing the noise addition:

            \[
            x_{t-1} \sim \mathcal{N}\left( \mu_t, \sigma_t^2 \, \mathbf{I} \right)
            \]

            where:

            - \( \sigma_t^2 = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \, \beta_t \)

            To enhance the quality of generated images, especially in class-conditional generation, I employed classifier-free guidance. This technique involves generating both conditional and unconditional predictions and combining them to strengthen the conditioning signal:

            \[
            \epsilon_{\text{guided}} = \epsilon_{\text{uncond}} + w \left( \epsilon_{\text{cond}} - \epsilon_{\text{uncond}} \right)
            \]

            where \( w \) is the guidance scale controlling the influence of the conditioning. This approach amplifies the difference between conditional and unconditional predictions, leading to samples that better adhere to the desired class characteristics.

            Throughout the implementation, careful attention was paid to the mathematical details of the diffusion processes. Precomputing terms like \( \bar{\alpha}_t \), \( \sqrt{\bar{\alpha}_t} \), and \( \sqrt{1 - \bar{\alpha}_t} \) was essential for computational efficiency. Random sampling of time steps during training ensured that the model learned to predict the noise across all stages of the diffusion process.

            By integrating these mathematical principles into the U-Net architecture and implementing the DDPM framework, I successfully developed a model capable of generating high-quality images through the reverse diffusion process. This work not only deepened my understanding of diffusion-based generative models but also demonstrated the practical application of complex mathematical concepts in deep learning.

            Upon examining the provided code, particularly the implementation of the Denoising Diffusion Probabilistic Model (DDPM) using a U-Net architecture, I observe that Classifier-Free Guidance (CFG) is utilized to enhance the quality and fidelity of the generated images, especially in class-conditional image generation tasks. The `ConditionalUNet` class is designed to accept the noisy input image `x_t`, the class label `c`, and the normalized time step `t` as inputs. This network integrates time and class embeddings through fully connected layers (`FCBlock`), conditioning the model's predictions on both the progression of the diffusion process and the desired class label. During training, the model learns to predict the added noise `epsilon` given these inputs, and a masking mechanism (`mask`) is employed to randomly omit conditioning information with a probability `p_uncond`. This mechanism is crucial for implementing CFG, as it allows the model to perform both conditional and unconditional generation by sometimes providing and sometimes withholding the class information during training.

            In the reverse diffusion process, during sampling, the model computes both conditional and unconditional predictions. The conditional prediction (`eps_cond`) is generated when the model is conditioned on the class label, while the unconditional prediction (`eps_uncond`) is produced without any class conditioning (when the class label is masked out). These two predictions are then combined using a guidance scale (`guidance_scale`) to produce the final guided prediction (`eps_theta`). This combination is mathematically expressed as:

            \[
            \epsilon_{\text{guided}} = \epsilon_{\text{uncond}} + w \left( \epsilon_{\text{cond}} - \epsilon_{\text{uncond}} \right)
            \]

            where \( w \) is the guidance scale controlling the strength of the conditioning signal. This formula effectively amplifies the influence of the class conditioning by emphasizing the difference between the conditional and unconditional predictions, pushing the generated samples toward the desired class characteristics.

            I believe that Classifier-Free Guidance is functioning as a technique to improve the quality of generated images in conditional generative models without relying on an external classifier for guidance. Instead, it leverages the generative model's own capacity to understand both general data distributions and class-specific features by training it to perform both conditional and unconditional generation. During sampling, CFG allows the model to balance diversity and fidelity by adjusting the guidance scale, enhancing the conditioning signal without the need for an external classifier. This results in images that are not only high in quality but also closely aligned with the specified class labels, leading to more cohesive and effective generative modeling. By integrating conditional and unconditional predictions within the same model and modulating the influence of conditioning, CFG simplifies the training and sampling processes while improving the overall performance of the model in class-conditional image generation tasks.

            We can clearly see that when guidance_scale = 0, the final image does not successfully form numbers, while when guidance_scale = 5 and 10, the final image successfully forms numbers and the actual effect is not much different.
        </p>
         <!-- Images for Part 2 -->
        <!-- Group for guidance_scale=0 -->
        <h3>guidance_scale=0</h3>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/0.1.png" alt="guidance_scale=0 image 1">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/0.2.png" alt="guidance_scale=0 image 2">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/0.3.png" alt="guidance_scale=0 image 3">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/0.4.png" alt="guidance_scale=0 image 4">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/0.5.png" alt="guidance_scale=0 image 5">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/0.6.png" alt="guidance_scale=0 image 6">
            </div>
        </div>

        <!-- Group for guidance_scale=5 -->
        <h3>guidance_scale=5</h3>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/5.1.png" alt="guidance_scale=5 image 1">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/5.2.png" alt="guidance_scale=5 image 2">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/5.3.png" alt="guidance_scale=5 image 3">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/5.4.png" alt="guidance_scale=5 image 4">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/5.5.png" alt="guidance_scale=5 image 5">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/5.6.png" alt="guidance_scale=5 image 6">
            </div>
        </div>

        <!-- Group for guidance_scale=10 -->
        <h3>guidance_scale=10</h3>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/10.1.png" alt="guidance_scale=10 image 1">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/10.2.png" alt="guidance_scale=10 image 2">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/10.3.png" alt="guidance_scale=10 image 3">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/10.4.png" alt="guidance_scale=10 image 4">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/10.5.png" alt="guidance_scale=10 image 5">
            </div>
            <div class="image-wrapper">
                <img src="./picture/part2/10.6.png" alt="guidance_scale=10 image 6">
            </div>
        </div>
    </section>

    <footer>
        <p>&copy; 2024 Guanyou Li. All Rights Reserved.</p>
    </footer>

</body>
</html>

