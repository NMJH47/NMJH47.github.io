<!DOCTYPE html> 
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project 5b - Guanyou Li</title>
    <!-- Include MathJax for rendering mathematical formulas -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f0f0f0;
        }
        h1, h2, h3 {
            text-align: center;
        }
        section {
            margin: 20px auto;
            padding: 20px;
            max-width: 1200px;
            background-color: #ffffff;
            border-radius: 10px;
        }
        p {
            text-align: justify;
            line-height: 1.6;
        }
        .image-container {
            text-align: center;
            margin-top: 20px;
        }
        .image-wrapper {
            display: inline-block;
            margin: 10px;
        }
        .image-wrapper img {
            max-width: 80%;
            height: auto;
        }
        .caption {
            text-align: center;
            font-style: italic;
            color: #555;
        }
        footer {
            text-align: center;
            margin: 40px 0;
            font-size: 14px;
            color: #aaa;
        }
    </style>
</head>
<body>

    <h1>Project 5b - Guanyou Li</h1>

    <section>
        <h2>Part 1</h2>
        <p>
            I aimed to implement and train a single-step denoising U-Net to remove Gaussian noise from images. The U-Net architecture is well-suited for this task due to its ability to capture both local and global features through its encoder-decoder structure with skip connections.

            The implementation starts by defining the fundamental components of the U-Net model. The encoder part of the network is responsible for downsampling the input image to capture high-level features. This is achieved using convolutional layers with stride and padding to reduce the spatial dimensions while increasing the depth of the feature maps. Mathematically, a convolution operation applies a kernel over the input feature map to produce an output feature map, effectively extracting features such as edges and textures.

            In the encoder, each convolutional layer is followed by a non-linear activation function, specifically the Gaussian Error Linear Unit (GELU). The GELU activation function is defined as:

            \[
            \text{GELU}(x) = x \cdot \Phi(x)
            \]

            where \( \Phi(x) \) is the cumulative distribution function of the standard normal distribution. This activation function allows the network to weight inputs by their significance, improving the model's ability to learn complex patterns.

            The decoder part of the U-Net performs upsampling to restore the original spatial dimensions of the image. Transposed convolutional layers, also known as deconvolutions, are used for upsampling. These layers mathematically reverse the downsampling process by learning how to map low-resolution feature maps back to higher resolutions. The decoder mirrors the encoder's structure, and at each level, it incorporates skip connections from the corresponding encoder layer. These skip connections concatenate the feature maps from the encoder to the decoder, preserving spatial information that might have been lost during downsampling.

            To train the denoising model, I generated noisy images by adding Gaussian noise to clean images. This process is mathematically represented as:

            \[
            \tilde{x} = x + \sigma \cdot \epsilon
            \]

            where \( x \) is the original clean image, \( \sigma \) is the standard deviation of the noise, and \( \epsilon \) is a random tensor sampled from a standard normal distribution \( \mathcal{N}(0, 1) \). The noisy image \( \tilde{x} \) serves as the input to the model, while the original image \( x \) is the target output.

            The loss function used for training is the Mean Squared Error (MSE), which measures the average squared difference between the estimated values and the actual value:

            \[
            \mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} \left\| f(\tilde{x}_i) - x_i \right\|^2
            \]

            where \( N \) is the number of samples, \( f(\tilde{x}_i) \) is the model's output for the \( i \)-th noisy input, and \( x_i \) is the corresponding clean image. The MSE loss encourages the model to produce outputs that are as close as possible to the original images.

            For optimization, I used the Adam optimizer, which is an adaptive learning rate optimization algorithm that computes individual learning rates for different parameters. It combines the advantages of two other extensions of stochastic gradient descent: adaptive gradient algorithm (AdaGrad) and root mean square propagation (RMSProp).

            Throughout the training process, I monitored the model's performance by visualizing the denoised images alongside the original and noisy images. This qualitative assessment helped in understanding how well the model was learning to remove noise. Additionally, testing the model with different levels of noise provided insights into its robustness and generalization capabilities.

            Implementing this denoising U-Net involved a deep understanding of convolutional neural networks and their mathematical foundations. By leveraging convolutional operations, activation functions like GELU, and optimization algorithms, the model effectively learned to map noisy images back to their clean counterparts. This project not only demonstrated the practical applications of deep learning in image restoration but also provided a solid foundation for more advanced tasks such as image segmentation and generative modeling.
        </p>
        <!-- Images for Part 1 -->
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part1/1.1.png" alt="Part 1 Image 1">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part1/1.2.png" alt="Part 1 Image 2">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part1/1.3.png" alt="Part 1 Image 3">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part1/1.4.png" alt="Part 1 Image 4">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part1/1.5.png" alt="Part 1 Image 5">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part1/1.6.png" alt="Part 1 Image 6">
            </div>
        </div>
    </section>

    <section>
        <h2>Part 2</h2>
        <p>
            <strong>Part 2.1: Time Conditioning to UNet</strong>

           In implementing the diffusion model, I delved deeply into the mathematical foundations of stochastic processes and denoising score matching. The core idea revolves around modeling the data distribution by progressively adding Gaussian noise and then training a model to reverse this process. This approach allows the model to generate new data samples that closely resemble the original data distribution.

**Forward Diffusion Process**

The forward diffusion process incrementally adds Gaussian noise to the data over \( T \) timesteps. At each timestep \( t \), noise is added according to a predefined schedule. Mathematically, this process is defined as:

\[
q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}\left( \mathbf{x}_t; \sqrt{\alpha_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I} \right),
\]

where \( \alpha_t = 1 - \beta_t \) and \( \beta_t \) is the noise variance at timestep \( t \). By composing these transitions, the noisy data at any timestep \( t \) can be expressed directly in terms of the original data \( \mathbf{x}_0 \):

\[
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon},
\]

with \( \bar{\alpha}_t = \prod_{s=1}^t \alpha_s \) and \( \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \). This equation shows that the data becomes increasingly noisy as \( t \) increases.

**Noise Schedule**

The noise schedule determines how \( \beta_t \) changes over time. A linear schedule interpolating between a starting value \( \beta_{\text{start}} \) and an ending value \( \beta_{\text{end}} \) is used:

\[
\beta_t = \beta_{\text{start}} + \frac{t}{T} \left( \beta_{\text{end}} - \beta_{\text{start}} \right).
\]

This schedule affects the cumulative product \( \bar{\alpha}_t \), which is crucial for both the forward diffusion and reverse denoising processes.

**Reverse Diffusion Process**

The goal is to learn the reverse of the forward process, denoted as \( p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \), which ideally inverts the noise addition:

\[
p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}\left( \mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \sigma_t^2 \mathbf{I} \right).
\]

Here, \( \boldsymbol{\mu}_\theta \) is the mean predicted by a neural network conditioned on \( \mathbf{x}_t \) and \( t \), and \( \sigma_t^2 \) is the variance, which can be set based on the forward process.

**Training Objective**

Training aims to minimize the difference between the true noise \( \boldsymbol{\epsilon} \) and the noise predicted by the neural network \( \boldsymbol{\epsilon}_\theta \). The simplified loss function used is:

\[
\mathcal{L} = \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}, t} \left[ \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right\|^2 \right].
\]

This loss is derived from variational lower bounds and is a form of denoising score matching, encouraging the model to accurately predict the noise at each timestep.

**Time-Conditioned Neural Network**

The neural network \( \boldsymbol{\epsilon}_\theta \) is designed to predict the noise present in the data at each timestep. It is conditioned on \( t \) to account for the time-dependent nature of the noise addition. The network architecture includes:

- **Time Embedding**: Encoding the timestep \( t \) into a higher-dimensional representation to provide temporal information to the network.
- **UNet Architecture**: A convolutional network with downsampling and upsampling paths, capturing multi-scale spatial features and allowing skip connections for efficient gradient flow.

**Sampling Procedure**

To generate new samples, the process starts with Gaussian noise \( \mathbf{x}_T \) and iteratively applies the learned reverse transitions:

\[
\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right) + \sigma_t \mathbf{z},
\]

where \( \mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \) if \( t > 1 \) and \( \mathbf{z} = \mathbf{0} \) if \( t = 1 \), and \( \sigma_t = \sqrt{\beta_t} \). This equation updates the sample by removing the estimated noise and adding a small amount of random noise to maintain stochasticity, gradually refining the sample towards the data distribution.

**Mathematical Derivation of Key Terms**

- **Cumulative Product of Alphas**:

  \[
  \bar{\alpha}_t = \prod_{s=1}^t \alpha_s.
  \]

- **Variance Terms**:

  \[
  \beta_t = 1 - \alpha_t,
  \]
  
  \[
  \tilde{\beta}_t = \beta_t \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t}.
  \]

- **Mean of Reverse Process**:

  The mean \( \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) \) for the reverse process is derived based on the posterior distribution \( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \):

  \[
  \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right).
  \]

**Theoretical Foundations**

This approach is grounded in several theoretical concepts:

- **Stochastic Differential Equations (SDEs)**: The diffusion process can be viewed as a discretization of an SDE, connecting discrete timesteps to continuous-time diffusion models.
- **Score Matching**: The model implicitly estimates the score function \( \nabla_{\mathbf{x}} \log q_t(\mathbf{x}) \), which is the gradient of the log probability density of the noisy data at timestep \( t \).
- **Variational Inference**: The training objective is derived from maximizing a variational lower bound on the data likelihood, framing the diffusion model within probabilistic modeling and allowing for principled training.

**Practical Implementation Details**

- **Random Timestep Selection**: During training, timesteps \( t \) are randomly sampled for each data point to ensure the model learns to denoise across all stages of the diffusion process.
- **Normalization of Timestep**: The timestep \( t \) is normalized (e.g., \( t / T \)) before being input into the network, which helps in stabilizing training and improving performance.
- **Loss Computation**: The mean squared error loss between the true noise \( \boldsymbol{\epsilon} \) and the predicted noise \( \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \) guides the model to accurately estimate the noise component at each timestep.

**Conclusion**

By integrating these mathematical concepts, the model effectively learns to reverse the diffusion process, transforming noisy data back into samples from the original data distribution. The time-conditioned neural network captures both the temporal dynamics of noise addition and the spatial structures within the data, allowing for accurate noise prediction and high-quality sample generation. This method leverages the interplay between stochastic processes, neural network approximation, and probabilistic modeling to model complex data distributions and generate new, realistic data samples.
        </p>
         <!-- Images for Part 2 -->
        <!-- Group for guidance_scale=0 -->
        <h3>guidance_scale=0</h3>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/0.1.png" alt="guidance_scale=0 image 1">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/0.2.png" alt="guidance_scale=0 image 2">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/0.3.png" alt="guidance_scale=0 image 3">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/0.4.png" alt="guidance_scale=0 image 4">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/0.5.png" alt="guidance_scale=0 image 5">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/0.6.png" alt="guidance_scale=0 image 6">
            </div>
        </div>

        <!-- Group for guidance_scale=5 -->
        <h3>guidance_scale=5</h3>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/5.1.png" alt="guidance_scale=5 image 1">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/5.2.png" alt="guidance_scale=5 image 2">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/5.3.png" alt="guidance_scale=5 image 3">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/5.4.png" alt="guidance_scale=5 image 4">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/5.5.png" alt="guidance_scale=5 image 5">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/5.6.png" alt="guidance_scale=5 image 6">
            </div>
        </div>

        <!-- Group for guidance_scale=10 -->
        <h3>guidance_scale=10</h3>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/10.1.png" alt="guidance_scale=10 image 1">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/10.2.png" alt="guidance_scale=10 image 2">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/10.3.png" alt="guidance_scale=10 image 3">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/10.4.png" alt="guidance_scale=10 image 4">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/10.5.png" alt="guidance_scale=10 image 5">
            </div>
        </div>
        <div class="image-container">
            <div class="image-wrapper">
                <img src="./picture/part2/10.6.png" alt="guidance_scale=10 image 6">
            </div>
        </div>
    </section>

    <footer>
        <p>&copy; 2024 Guanyou Li. All Rights Reserved.</p>
    </footer>

</body>
</html>
